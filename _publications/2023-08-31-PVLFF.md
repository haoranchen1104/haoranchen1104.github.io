---
title: "Panoptic Vision-Language Feature Fields"
collection: publications
permalink: /publication/PVLFF
excerpt: 'PVLFF, a method based on neural fields for zero-shot open-vocabulary panoptic segmentation in 3D scenes.'
date: 2023-08-31
venue: 'preprint'
paperurl:
citation: 'Haoran Chen , Kenneth Blomqvist , Francesco Milano and Roland Siegwart. &quot;Panoptic Vision-Language Feature Fields.&quot; <i>preprint</i>. 2023'
codeurl: 'https://github.com/ethz-asl/autolabel'
---

Recently, methods have been proposed for 3D open-vocabulary semantic segmentation. Such methods are able to segment scenes into arbitrary classes given at run-time using their text description. In this paper, we propose to our knowledge the first algorithm for open-vocabulary panoptic segmentation, simultaneously performing both semantic and instance segmentation. Our algorithm, Panoptic Vision-Language Feature Fields (PVLFF) learns a feature field of the scene, jointly learning vision-language features and hierarchical instance features through a contrastive loss function from 2D instance segment proposals on input frames. Our method achieves comparable performance against the state-of-the-art close-set 3D panoptic systems on the HyperSim, ScanNet and Replica dataset and outperforms current 3D open-vocabulary systems in terms of semantic segmentation. We additionally ablate our method to demonstrate the effectiveness of our model architecture. Our code will be available at https://github.com/ethz-asl/autolabel.

