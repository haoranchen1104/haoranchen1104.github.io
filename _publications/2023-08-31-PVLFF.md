---
title: "Panoptic Vision-Language Feature Fields"
header:
    teaser: /images/publication/PVLFF/teaser.png
collection: publications
permalink: /publication/PVLFF
excerpt: 'In this paper, we proposed a open-vocabulary panoptic system based on neural fields for scene understanding. Our method implicitly reconstructs the scene geometry from 2D images and simultaneously gains panoptic informaiton from 2D proposals computed by off-the-shelf 2D networks.'
date: 2023-08-31
venue: 'preprint'
paperurl: 'https://arxiv.org/abs/2309.05448'
citation: 'Haoran Chen , Kenneth Blomqvist , Francesco Milano and Roland Siegwart. &quot;Panoptic Vision-Language Feature Fields.&quot; <i>preprint</i>. 2023'
codeurl: 'https://github.com/ethz-asl/autolabel'
---

![Overview of PVLFF](https://haoranchen1104.github.io/images/publication/PVLFF/teaser.png)

**Abstract**
> Recently, methods have been proposed for 3D open-vocabulary semantic segmentation. Such methods are able to segment scenes into arbitrary classes given at run-time using their text description. In this paper, we propose to our knowledge the first algorithm for open-vocabulary panoptic segmentation, simultaneously performing both semantic and instance segmentation. Our algorithm, Panoptic Vision-Language Feature Fields (PVLFF) learns a feature field of the scene, jointly learning vision-language features and hierarchical instance features through a contrastive loss function from 2D instance segment proposals on input frames. Our method achieves comparable performance against the state-of-the-art close-set 3D panoptic systems on the HyperSim, ScanNet and Replica dataset and outperforms current 3D open-vocabulary systems in terms of semantic segmentation. We additionally ablate our method to demonstrate the effectiveness of our model architecture. Our code will be available at [https://github.com/ethz-asl/autolabel](https://github.com/ethz-asl/autolabel).

